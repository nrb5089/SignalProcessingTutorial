{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dry Run Tensorflow (Not Quite 'Hello, World')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dIo3CKPhtIi"
   },
   "source": [
    "Ensure the following lines output correctly to ensure the python kernel in use has tensorflow installed.  It's recommended for folks without a working environment on their personal computer to utilize Google Colab https://colab.research.google.com  which allows the use of Jupyter notebooks within the browser (Google's cloud kernel for python).  Login with your Google credentials and choose from the upper-right hand side 'File' >> 'Upload Notebook...'.  The notebook will be saved to your Google drive and can be edited through the IDE within the browser.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4ZTgdjGqhdub"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "hello = tf.constant('hello,tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "NFyEuqUYhduf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "print(hello)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Ku-7FzHlIzD"
   },
   "source": [
    "This should produce just information about the tensor 'hello' that we defined. If the output value of the tensor is required, then a ```session``` all Tensorflow operations must be run in a ```tf.Session()``` (which runs within the C++ graph).  This is part of the way it is able to efficiently utilize GPUs for parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q96F41T5hduj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'hello,tensorflow'\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(hello))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eNehOTkvlTuN"
   },
   "source": [
    "If this printed the string that we previously defined ```'hello, tensorflow'```, then we are in business. If you did not see the output in parentheses, then please check the python kernel or Anaconda installation.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A brief overview of linear algebra and it's connections to tensor arithmetic within tensorflow is given next along with code modules that demonstrate the implementation of such operations.  Please have ```tensorflow``` or ```tensorflow-gpu``` istalled in the python kernel you're using.  If you're using an Anaconda environment, activate the ```conda activate <environment> ``` if it's virtual, then call ```jupyter notebook``` in the terminal.  All arithmetic demos (python code) is done within tensosorflow variables to get the reader acquainted with doing things within a session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Review of Linear Algebra $\\label{ssec:reviewlinalg}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key building block in Machine Learning and Deep Learning is linear algebra.  As a quick review, some basic linear algebraic operations are described below for $u\\in \\mathbb{C}^{M\\times 1}$, $v \\in\\mathbb{C}^{N\\times 1}$, and $A \\in \\mathbb{C}^{M\\times N}$ and $B \\in \\mathbb{C}^{N\\times O}$.  First let's digress, that was some fancy notation, but all it means for our purposes is that $u$ and $v$ are column vectors of length $M$.  The reason for writing $M\\times 1$ is to denote that we are working with tensors of maximum dimensionality $2$.  Furthermore $A$ is a matrix with length $M$ rows and $N$ columns.  The $\\mathbb{C}$ just means that all elements belong to the set of complex numbers (more general than just saying just $\\mathbb{R}$ since $\\mathbb{R} \\subset \\mathbb{C}$ (set of real numbers is a subset of the set of complex numbers).  All operations apply to real numbers, so for $c\\in \\mathbb{R}$ the conjugate $c^* = c$ and only the transpose operation must be applied.\n",
    "\n",
    "The inner product \\ref{eq:innerproduct} (or dot product) of two vectors $u$ and $v$ (where $M=N$) is a scalar $\\alpha\\in \\mathbb{C}$. The subscript $H$ denotes the Hermitian conjugate  (transpose and complex conjugate denoted by *).  Note that $u$ and $v$ must be the same length.  Recall that the transpose of a vector is the transformation of a column/row vector to a row/column vector \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\label{eq:innerproduct}\n",
    "u^Hv = (u^T)^*v = (u^*)^Tv = \\alpha \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{align*}\n",
    "u = \\begin{bmatrix}u_1\\\\ u_2\\\\ \\vdots \\\\ u_M\\end{bmatrix} & & u^H = \\begin{bmatrix} u_1^* & u_2^*& \\dots& u_M^*\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u = [[7.722453  +9.282378j ]\n",
      " [8.828606  +4.1114306j]\n",
      " [4.840288  +7.2938814j]\n",
      " [3.4835315 +7.921218j ]\n",
      " [0.32647252+1.5788436j]\n",
      " [7.4810467 +8.26948j  ]\n",
      " [0.9089947 +4.7931824j]\n",
      " [6.3937426 +2.121091j ]\n",
      " [2.0608914 +6.6455317j]\n",
      " [5.6919823 +2.3149025j]]\n",
      "\n",
      "v = [[3.0895472+8.533325j ]\n",
      " [5.8567   +7.4076786j]\n",
      " [7.43654  +6.58187j  ]\n",
      " [8.602978 +8.50073j  ]\n",
      " [2.4093413+0.4243815j]\n",
      " [3.5103667+7.5109982j]\n",
      " [3.7673569+4.712038j ]\n",
      " [0.6004691+8.979704j ]\n",
      " [8.511828 +3.152585j ]\n",
      " [8.005124 +3.0215967j]]\n",
      "\n",
      "alpha = [[596.3167+32.084274j]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf #import tensorflow library\n",
    "M = 10 #set length of vectors\n",
    "u_real = tf.random.uniform(shape = [M,1],minval = 0, maxval = 10, dtype = tf.float32) #generate random real part\n",
    "u_imag = tf.random.uniform(shape = [M,1],minval = 0, maxval = 10, dtype = tf.float32) #generate random imaginary part\n",
    "u = tf.complex(u_real,u_imag) #combine into a complex-valued vector\n",
    "\n",
    "v_real = tf.random.uniform(shape = [M,1],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "v_imag = tf.random.uniform(shape = [M,1],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "v = tf.complex(v_real,v_imag)\n",
    "\n",
    "alpha = tf.matmul(tf.transpose(tf.conj(u)),v) #matmul (matrix multiplication) interprets these as two column vectors\n",
    "with tf.Session() as sess: #initialize tensorflow session\n",
    "    u_o, v_o, alpha_o = sess.run([u,v,alpha]) #run operations\n",
    "    print('u = ' + str(u_o) + '\\n')\n",
    "    print('v = ' + str(v_o) + '\\n')\n",
    "    print('alpha = ' + str(alpha_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inner product of a vector and itself is equivalent to the $l_2$-norm squared $u^Hu=||u||_2^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outer product \\ref{eq:outerproduct} of two vectors $u$ and $v$ (potentially $M\\neq N$, and that's ok) outputs to a matrix $A\\in \\mathbb{C}^{M\\times N}$.  Observe that a matrix is produced at the output, this is also called the tensor product in that particular realm of mathematics.  The magic of it is that it can map to higher dimensional spaces in tensor-space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\\begin{equation}\\label{eq:outerproduct}\n",
    "uv^H = A\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u = [[1.4175344 +2.3486602j ]\n",
      " [3.7481391 +8.855494j  ]\n",
      " [3.774724  +0.10832429j]\n",
      " [9.74114   +5.4172506j ]\n",
      " [2.0374691 +5.512577j  ]\n",
      " [6.353408  +8.133069j  ]\n",
      " [0.13003588+9.93559j   ]\n",
      " [4.5236754 +1.0869288j ]\n",
      " [0.9468186 +3.0854392j ]\n",
      " [0.83509445+2.8092015j ]]\n",
      "\n",
      "v = [[2.7281356+0.6795788j]\n",
      " [4.627669 +2.211783j ]\n",
      " [4.6510816+6.526034j ]\n",
      " [3.9499831+7.780737j ]\n",
      " [3.767426 +8.156301j ]]\n",
      "\n",
      "alpha = [[ 5.4633255 +5.444137j  11.754606  +7.733544j  21.920506  +1.6729326j\n",
      "  23.873545  -1.7522936j 24.496834  -2.7134323j]\n",
      " [16.243437 +21.611832j  36.931576 +32.690224j  75.22415  +16.727139j\n",
      "  83.70735   +5.8157654j 86.3489    +2.7914658j]\n",
      " [10.371574  -2.2696989j 17.707762  -7.8475814j 18.263477 -24.130152j\n",
      "  15.752939 -28.942255j  15.104519 -30.379679j ]\n",
      " [30.2566    +8.159122j  57.060555  +3.5239544j 80.66     -38.374935j\n",
      "  80.62755  -54.395203j  80.88374  -59.04258j  ]\n",
      " [ 9.304723 +13.654437j  21.621357 +21.003942j  45.451702 +12.342854j\n",
      "  50.93988   +5.9215746j 52.63825   +4.150017j ]\n",
      " [22.86002  +17.870472j  47.390053 +23.584791j  82.6269    -3.6349869j\n",
      "  88.37712  -17.308712j  90.27174  -21.17957j  ]\n",
      " [ 7.1067715+27.017267j  22.577131 +45.691006j  65.4448   +45.36262j\n",
      "  77.819855 +38.233635j  81.52756  +36.370987j ]\n",
      " [13.079854  -0.1089046j 23.338123  -4.9754415j 28.133318 -24.466265j\n",
      "  26.32555  -30.904179j  25.90793  -32.801533j ]\n",
      " [ 4.6798487 +7.774059j  11.205885 +12.184234j  24.539412  +8.171659j\n",
      "  27.746908  +4.820487j  28.732838  +3.9016266j]\n",
      " [ 4.1873245 +7.09637j   10.077885 +11.153007j  22.217037  +7.6159706j\n",
      "  25.156267  +4.598648j  26.058847  +3.7721777j]]\n"
     ]
    }
   ],
   "source": [
    "#The outer product \n",
    "import tensorflow as tf #import tensorflow library\n",
    "M = 10\n",
    "N = 5\n",
    "\n",
    "u_real = tf.random.uniform(shape = [M,1],minval = 0, maxval = 10, dtype = tf.float32) #generate random real part\n",
    "u_imag = tf.random.uniform(shape = [M,1],minval = 0, maxval = 10, dtype = tf.float32) #generate random imaginary part\n",
    "u = tf.complex(u_real,u_imag) #combine into a complex-valued vector\n",
    "\n",
    "v_real = tf.random.uniform(shape = [N,1],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "v_imag = tf.random.uniform(shape = [N,1],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "v = tf.complex(v_real,v_imag)\n",
    "\n",
    "alpha = tf.matmul(u,tf.transpose(tf.conj(v))) #matmul (matrix multiplication) interprets these as two column vectors\n",
    "with tf.Session() as sess: #initialize tensorflow session\n",
    "    u_o, v_o, alpha_o = sess.run([u,v,alpha]) #run operations\n",
    "    print('u = ' + str(u_o) + '\\n')\n",
    "    print('v = ' + str(v_o) + '\\n')\n",
    "    print('alpha = ' + str(alpha_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix-vector product \\ref{eq:matrixvectorproduct} of $A$ and $v$ is perhaps the most applicable to ML/DL in that it defines the transformation between layers of the multi-layer perceptron (MLP) layers previous to the activation function.  The output is a vector $u$, and please note that this NOT a reversible operation of the outer product unless $u$ and $v$ meet certain conditions (orthonormality $v^Hv = 1$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\label{eq:matrixvectorproduct}\n",
    "Av = u\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [[8.142372  +9.510589j   8.440131  +3.650186j   7.5445232 +8.212906j\n",
      "  3.9699113 +5.343623j   9.988331  +3.4504747j ]\n",
      " [3.0326414 +8.998622j   3.893143  +8.159142j   6.1330533 +2.3954666j\n",
      "  4.8269415 +6.466756j   9.4401    +9.767322j  ]\n",
      " [4.16905   +8.80947j    5.112467  +1.382519j   5.148793  +2.709645j\n",
      "  9.272905  +0.30922413j 0.1789248 +3.601383j  ]\n",
      " [1.1851394 +0.8877075j  9.925453  +5.8170853j  9.768124  +1.4073241j\n",
      "  5.6646633 +0.4477334j  1.099261  +4.08144j   ]\n",
      " [0.59557676+4.353899j   8.625451  +6.2170744j  9.963778  +2.2882676j\n",
      "  9.310974  +9.192639j   4.4949875 +6.3704896j ]\n",
      " [9.472601  +6.825559j   3.6200786 +4.1345444j  4.508277  +1.0914028j\n",
      "  1.297183  +2.9940498j  4.1749516 +4.4151487j ]\n",
      " [0.52348495+7.5420914j  2.3730981 +7.4080634j  4.9783716 +3.658613j\n",
      "  2.6525211 +1.9388068j  9.1264105 +3.05282j   ]\n",
      " [7.3440337 +9.1800165j  9.758996  +0.48946738j 8.904049  +2.2649336j\n",
      "  3.0926836 +2.7547884j  5.0664196 +7.764623j  ]\n",
      " [8.006268  +8.827463j   1.4945018 +3.8823628j  4.3642807 +5.3178015j\n",
      "  3.118807  +8.349214j   6.5225196 +2.9640365j ]\n",
      " [7.324574  +2.4026942j  4.626274  +8.771503j   9.708717  +2.4587727j\n",
      "  8.900623  +3.0565047j  7.832779  +3.126402j  ]]\n",
      "\n",
      "v = [[5.769802  +3.626089j ]\n",
      " [2.9150796 +8.734231j ]\n",
      " [8.059367  +7.38178j  ]\n",
      " [7.5673447 +2.5188375j]\n",
      " [0.33225656+2.160194j ]]\n",
      "\n",
      "u = [[ 17.840904+363.80045j]\n",
      " [-41.025696+270.016j  ]\n",
      " [ 78.104836+201.7552j ]\n",
      " [ 83.36839 +217.90329j]\n",
      " [ 56.93808 +317.5765j ]\n",
      " [ 26.747349+195.88695j]\n",
      " [-57.371784+196.05424j]\n",
      " [ 89.67575 +292.40353j]\n",
      " [-21.11392 +265.52103j]\n",
      " [ 86.02269 +261.3909j ]]\n"
     ]
    }
   ],
   "source": [
    "#The matrix-vector product\n",
    "import tensorflow as tf #import tensorflow library\n",
    "M = 10 \n",
    "N = 5 \n",
    "\n",
    "\n",
    "A_real = tf.random.uniform(shape = [M,N],minval = 0, maxval = 10, dtype = tf.float32) #generate random real part\n",
    "A_imag = tf.random.uniform(shape = [M,N],minval = 0, maxval = 10, dtype = tf.float32) #generate random imaginary part\n",
    "A = tf.complex(A_real,A_imag) #combine into a complex-valued vector\n",
    "\n",
    "v_real = tf.random.uniform(shape = [N,1],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "v_imag = tf.random.uniform(shape = [N,1],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "v = tf.complex(v_real,v_imag)\n",
    "\n",
    "u = tf.matmul(A,v) #matmul (matrix multiplication) interprets these as two column vectors\n",
    "with tf.Session() as sess: #initialize tensorflow session\n",
    "    A_o, v_o, u_o = sess.run([A,v,u]) #run operations\n",
    "    print('A = ' + str(A_o) + '\\n')\n",
    "    print('v = ' + str(v_o) + '\\n')\n",
    "    print('u = ' + str(u_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix-matrix product where $A \\in \\mathbb{C}^{M\\times N}$ and $B \\in \\mathbb{C}^{N\\times O}$ is also defined in \\ref{eq:matrixmatrixproduct} where the output $C \\in \\mathbb{C}^{M\\times O}$.  One major trend the reader should observe is the emphasis on the shape of the matrices (tensors and vectors too).  The shapes have been explicitly and arguably redundantly stated to coax the reader into using them as an analysis tool.  Often troubleshooting deep learning architectures requires knowledge of the tensors in question, and having knowledge of which dimensions fit where is essentially like using the tensors as bricks that fit together only one way.  In this example, the $N$ dimension is common between $A$ and $B$, so this is the only valid operation for this set of matrices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\label{eq:matrixmatrixproduct}AB = C\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [[5.962435  +5.0638447j  2.2047496 +5.620837j   7.462798  +1.8054688j\n",
      "  8.399988  +8.578398j   1.4547348 +3.9830744j ]\n",
      " [4.0647945 +6.330906j   4.33467   +5.439999j   2.0182943 +1.9505191j\n",
      "  0.3112781 +2.2209203j  1.7629492 +5.5241346j ]\n",
      " [2.2885966 +2.7099717j  3.819977  +0.56179285j 1.7270017 +8.35644j\n",
      "  6.815363  +4.905711j   3.3338153 +3.6041236j ]\n",
      " [5.5664134 +6.99811j    6.832715  +9.787225j   6.49246   +1.3596535j\n",
      "  5.6302905 +9.195929j   0.09796143+1.0132158j ]\n",
      " [4.4858274 +2.7995062j  7.629677  +0.19276619j 5.819112  +2.4012601j\n",
      "  0.42616367+7.1710777j  0.8725178 +8.616665j  ]\n",
      " [7.645664  +9.237071j   5.404868  +9.0873575j  0.32995343+2.1813762j\n",
      "  0.7598281 +5.834117j   7.3987126 +5.0760403j ]\n",
      " [1.3333893 +4.127201j   1.8179882 +7.6980915j  8.002882  +0.23452282j\n",
      "  0.2611816 +8.001409j   0.11083007+4.1966558j ]\n",
      " [5.236335  +1.114322j   0.38264513+8.714579j   2.7431    +6.3738084j\n",
      "  4.449227  +1.2254035j  6.6296997 +3.4734023j ]\n",
      " [2.2339356 +1.6061604j  3.7844276 +5.579625j   0.92746615+4.7162256j\n",
      "  1.411736  +8.048606j   2.9483795 +1.7502844j ]\n",
      " [0.75228333+7.3266544j  2.9119074 +3.2425475j  5.7612906 +7.6299896j\n",
      "  6.562175  +7.5468693j  1.0848498 +5.0728235j ]]\n",
      "\n",
      "B = [[6.008365 +8.324061j  8.127118 +6.3953295j 8.20941  +1.4058149j]\n",
      " [6.851108 +4.942515j  7.5451875+4.0327225j 6.401453 +3.6839902j]\n",
      " [6.3222933+2.3639941j 2.8820288+8.672995j  8.832222 +3.7698376j]\n",
      " [2.2771633+2.7384996j 7.701044 +4.9904084j 6.583519 +2.3429751j]\n",
      " [5.519917 +6.7805967j 5.5842876+3.5358894j 3.5125208+9.897975j ]]\n",
      "\n",
      "C = [[ 5.6912071e-01+232.90778j  3.1808525e+01+335.88425j\n",
      "   9.5230431e+01+242.68346j]\n",
      " [-5.0415585e+01+196.02763j -2.6159376e+01+214.83893j\n",
      "  -8.9899549e+00+185.52026j]\n",
      " [ 1.8008764e+00+187.3113j  -5.7955551e+00+199.07112j\n",
      "   3.0524603e+01+217.37276j]\n",
      " [-7.2278948e+00+255.76703j  1.3919926e+01+359.0222j\n",
      "   1.0159596e+02+267.84583j]\n",
      " [ 1.3804332e+01+193.10529j  1.3187898e+01+249.6073j\n",
      "   2.7145802e+01+188.88753j]\n",
      " [-4.9732544e+01+316.23914j -1.0666008e+01+326.7057j\n",
      "   1.2671603e+01+316.42462j]\n",
      " [-5.1055286e+01+160.87677j -6.3993809e+01+264.31937j\n",
      "   4.4740133e-02+193.10094j]\n",
      " [ 3.8302746e+00+237.7612j   8.6882057e+00+226.47966j\n",
      "   2.7292982e+01+236.6537j ]\n",
      " [-2.1302265e+01+169.03413j -4.3312347e+01+195.56432j\n",
      "  -6.3674588e+00+202.76347j]\n",
      " [-6.8289207e+01+219.26262j -8.0423721e+01+295.5524j\n",
      "   3.8119364e+00+275.4146j ]]\n"
     ]
    }
   ],
   "source": [
    "#The matrix-matrix product\n",
    "import tensorflow as tf #import tensorflow library\n",
    "M = 10 \n",
    "N = 5 \n",
    "O = 3\n",
    "\n",
    "A_real = tf.random.uniform(shape = [M,N],minval = 0, maxval = 10, dtype = tf.float32) #generate random real part\n",
    "A_imag = tf.random.uniform(shape = [M,N],minval = 0, maxval = 10, dtype = tf.float32) #generate random imaginary part\n",
    "A = tf.complex(A_real,A_imag) #combine into a complex-valued vector\n",
    "\n",
    "B_real = tf.random.uniform(shape = [N,O],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "B_imag = tf.random.uniform(shape = [N,O],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "B = tf.complex(B_real,B_imag)\n",
    "\n",
    "C = tf.matmul(A,B) #matmul (matrix multiplication) interprets these as two column vectors\n",
    "with tf.Session() as sess: #initialize tensorflow session\n",
    "    A_o, B_o, C_o = sess.run([A,B,C]) #run operations\n",
    "    print('A = ' + str(A_o) + '\\n')\n",
    "    print('B = ' + str(B_o) + '\\n')\n",
    "    print('C = ' + str(C_o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common operation is the Haddamard product \\ref{eq:haddamard} which is simply the element wise product between matrices.  Both matrices must have the same shape for this to work $A,B,C \\in \\mathbb{C}^{M\\times N}$.  Note that this works for vectors of the same length too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\\label{eq:haddamard}\n",
    "A\\odot B = C\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A = [[1.1865389 +4.5987263j  0.41492105+4.931855j   8.465663  +3.3013475j\n",
      "  9.124151  +3.1236255j  7.145218  +9.654247j  ]\n",
      " [6.9156733 +5.3987074j  4.73562   +6.390016j   5.006281  +2.2519755j\n",
      "  4.2206597 +0.22459388j 8.518777  +2.6624966j ]\n",
      " [0.1384604 +0.6687522j  9.842396  +4.725276j   1.731633  +9.606599j\n",
      "  4.259905  +9.991009j   6.9442415 +7.9226074j ]\n",
      " [4.6812716 +3.2270455j  3.7900198 +4.625348j   6.759808  +8.49701j\n",
      "  0.06435513+3.6307764j  7.1792674 +4.4907j    ]\n",
      " [8.869297  +6.457511j   2.0815766 +6.2989197j  6.204653  +2.8696775j\n",
      "  0.26066184+8.066862j   1.9892335 +0.76267123j]\n",
      " [3.5762036 +6.8442845j  4.179302  +1.8015182j  4.4397736 +2.5804102j\n",
      "  0.75586677+2.2523093j  9.999324  +6.7210045j ]\n",
      " [3.1195378 +3.2408464j  7.224002  +2.697208j   5.093213  +1.9434369j\n",
      "  2.3799932 +3.5660899j  8.862294  +0.55757403j]\n",
      " [4.556489  +7.776904j   1.3988495 +5.379491j   0.3308022 +0.9294069j\n",
      "  0.41858912+4.1944733j  3.5791767 +5.1598787j ]\n",
      " [6.4214444 +1.1430156j  0.4601705 +2.5016701j  9.517259  +3.1166995j\n",
      "  9.228831  +2.5637758j  0.5809927 +0.7240534j ]\n",
      " [9.179438  +0.35589814j 3.8517523 +8.399863j   5.0090265 +5.6465745j\n",
      "  2.9602456 +7.800597j   2.5904512 +5.3573227j ]]\n",
      "\n",
      "B = [[3.6371112 +6.1967907j  1.8000221 +9.157199j   8.928767  +9.118671j\n",
      "  5.5598917 +7.610303j   3.99266   +5.776174j  ]\n",
      " [6.3648415 +9.073358j   5.55626   +9.902424j   2.1741676 +0.5580902j\n",
      "  2.8035605 +0.34539104j 2.5427294 +7.2102857j ]\n",
      " [4.0056396 +7.678485j   5.8004866 +4.1012955j  2.1295106 +0.47379613j\n",
      "  8.007531  +5.5700293j  4.2721725 +9.387298j  ]\n",
      " [6.5512266 +8.876823j   8.234735  +2.4564838j  3.4267569 +4.5014677j\n",
      "  4.235173  +4.407028j   8.244389  +7.6273623j ]\n",
      " [1.0171545 +2.9321635j  0.40827274+7.355281j   4.229697  +5.077757j\n",
      "  8.770945  +7.4178305j  6.4080048 +7.8661776j ]\n",
      " [0.54353714+8.202768j   5.632477  +9.335956j   3.4805477 +5.2089477j\n",
      "  8.261429  +6.3519907j  3.9032996 +7.345046j  ]\n",
      " [5.473338  +6.3827744j  1.0089171 +6.6245246j  6.3070393 +8.365543j\n",
      "  3.4956622 +8.83128j    5.7429037 +7.087373j  ]\n",
      " [1.9619024 +0.62294006j 1.1821282 +7.0196495j  4.2312155 +5.2837753j\n",
      "  1.589998  +1.7649591j  4.8545494 +6.419356j  ]\n",
      " [2.1095073 +2.5362682j  9.969068  +0.8514905j  4.2637243 +0.53913116j\n",
      "  4.6997967 +5.820798j   6.6220975 +0.93976736j]\n",
      " [9.369198  +6.166562j   8.391775  +6.5309334j  9.298246  +8.930717j\n",
      "  5.6584682 +6.8673086j  7.6200256 +0.9024763j ]]\n",
      "\n",
      "C = [[-24.18177    +24.078812j  -44.41511    +12.676963j\n",
      "   45.48403   +106.67256j    26.957556   +86.80457j\n",
      "  -27.236189   +79.81815j  ]\n",
      " [ -4.967238   +97.11029j   -36.964314   +82.39871j\n",
      "    9.627688    +7.690129j   11.755302    +2.0874405j\n",
      "    2.4635832  +68.192825j ]\n",
      " [ -4.580381    +3.7419465j  37.71093    +67.775475j\n",
      "   -0.86403865 +21.277794j  -21.53889   +103.73111j\n",
      "  -44.704876   +99.03441j  ]\n",
      " [  2.0221567  +62.695927j   19.847713   +47.398636j\n",
      "  -15.084797   +59.546246j  -15.728379   +15.660583j\n",
      "   24.936478   +91.78195j  ]\n",
      " [ -9.913032   +32.574516j  -45.480473   +17.882257j\n",
      "   11.672278   +43.643585j  -57.552364   +72.687546j\n",
      "    6.7477107  +20.534864j ]\n",
      " [-54.198284   +33.054893j    6.720929   +49.16479j\n",
      "    2.0116212  +32.107788j   -8.062109   +23.408552j\n",
      "  -10.335732   +99.67959j  ]\n",
      " [ -3.6113055  +37.649555j  -10.579301   +50.576836j\n",
      "   15.865189   +54.864826j  -23.173485   +33.48423j\n",
      "   46.943565   +66.012474j ]\n",
      " [  4.094841   +18.095945j  -36.108524   +16.17868j\n",
      "   -3.5110817   +5.6804056j  -6.737518    +7.407997j\n",
      "  -15.747807   +48.024895j ]\n",
      " [ 10.64709    +18.697706j    2.4573224  +25.331148j\n",
      "   38.89866    +18.419798j   28.45041    +65.76839j\n",
      "    3.1669486   +5.34075j  ]\n",
      " [ 83.809296   +59.940052j  -22.53591    +95.6453j\n",
      "   -3.8528004  +97.23744j   -36.818653   +64.46835j\n",
      "   14.904448   +43.16076j  ]]\n"
     ]
    }
   ],
   "source": [
    "#The Haddamard product\n",
    "import tensorflow as tf #import tensorflow library\n",
    "M = 10 \n",
    "N = 5 \n",
    "\n",
    "A_real = tf.random.uniform(shape = [M,N],minval = 0, maxval = 10, dtype = tf.float32) #generate random real part\n",
    "A_imag = tf.random.uniform(shape = [M,N],minval = 0, maxval = 10, dtype = tf.float32) #generate random imaginary part\n",
    "A = tf.complex(A_real,A_imag) #combine into a complex-valued vector\n",
    "\n",
    "B_real = tf.random.uniform(shape = [M,N],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "B_imag = tf.random.uniform(shape = [M,N],minval = 0, maxval = 10, dtype = tf.float32)\n",
    "B = tf.complex(B_real,B_imag)\n",
    "\n",
    "C = A*B \n",
    "with tf.Session() as sess: #initialize tensorflow session\n",
    "    A_o, B_o, C_o = sess.run([A,B,C]) #run operations\n",
    "    print('A = ' + str(A_o) + '\\n')\n",
    "    print('B = ' + str(B_o) + '\\n')\n",
    "    print('C = ' + str(C_o))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitioning to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Tensors are defined as any matrix with dimension greater than $2$.  Digital video data is an example of a 3D tensor where the axis are the length, width, and index of each frame (time samples).  This tensor could be denoted as $\\mathcal{A}\\in\\mathbb{R}^{L\\times W \\times T}$.  More generally it's written $\\mathcal{A}\\in \\mathbb{C}^{M_1\\times M_2 \\times \\dots \\times M_D}$ where $D$ denotes the dimension ($D=3$ for the video data case).  Datasets consisting of 2D image data is probably the most common in the world of DL.  A single grayscale image is actually a matrix $A \\in \\mathbb{R}^{L\\times W}$, and in a $\\textbf{batch}$ of images could be defined as a tensor $\\mathcal{A}\\in \\mathbb{R}^{B_s \\times L \\times W \\times 1}$  where $B_s$ denotes the batch size.  More than likely color images will be used and the last dimension $1$ for the grayscale case is simply as a placeholder for the case of a batch of color images $\\mathcal{B} \\in \\mathbb{R}^{B_s \\times L \\times W \\times 3}$ where the three indices represent $\\textbf{slices}$ of red, green, blue (RGB) values.  A slice is a two dimensional cross-section of a tensor, the $i^{th}$ image (red only) of the batch $\\mathcal{B}[i,:,:,0]$.  Note that the while the last dimension has dimension $3$ the indexing starts at $0$.  Furthermore, a $j^{th}$ tensor $\\textbf{fiber}$ of $\\mathcal{B}$ could be $\\mathcal{B}[i,:,j,0]$ which is actually a vector in tensor-space.\n",
    "\n",
    "The following example comes from the MNIST dataset, this is the \"Hello, World\" dataset for DL.  It consists of $70000$ grayscale $28\\times 28$ images which are split between training and testing data.  The data is imported as a numpy array which is valid for input data into the tensorflow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of B:(64, 28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   0.   3. 170. 253. 253. 253. 253. 253. 190.  35.\n",
      "   0.   0.   0.   0.   0.  39. 221. 253. 253. 133.   0.   0.   0.   0.]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "(all_data,_), (_,_) = mnist.load_data() #load dataset\n",
    "all_data = tf.convert_to_tensor(all_data,tf.float32) #convert to tensor, shape is [60000,28,28]\n",
    "all_data = tf.expand_dims(all_data,axis = 3) #add placeholder [60000,28,28,1] dimension for data input (more on this later)\n",
    "\n",
    "Bs = 64 #set batch size\n",
    "i = 0 #set index\n",
    "j = 12\n",
    "with tf.Session() as sess:\n",
    "    B = sess.run(all_data[0:Bs]) #extract batch through session\n",
    "    print('Shape of B:{}'.format(B.shape))\n",
    "    plt.imshow(B[i,:,:,0],cmap = 'gray')  #extract and disply slice.  Could be written more compactly as B[i]\n",
    "    plt.show()\n",
    "    print(B[i,:,j,0]) #extract fiber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may ask why wasn't the variable 'all_data' just indexed to produce an output.  This is because the reader is once again encouraged to get used to the idea of working out a tensorflow graph within a session.  This is vital to running models later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix operations in tensor-space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations in $\\ref{ssec:reviewlinalg}$ can be implemented with simpler functions in tensor-space.  These are often necessary when inducing linear transformations on particular slices. It's often necessary to carry extra dimensions.  For example a vector could have the shape [1,N,1] where the N values are the vector.  Likewise a matrix doesn't necesarily have to carry it's axis on adjacent dimensions for example the M by N matrix [M,1,N].  All are valid in tensor space.  \n",
    "\n",
    "It might be necessary to rearrange (transpose) the axis so that the required operation can take place.  <elaborate more on this>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Haddamard product and the sum of those elements is an inner product (dot product)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:[[ 4.]\n",
      " [-8.]\n",
      " [-2.]\n",
      " [ 5.]\n",
      " [ 1.]]\n",
      "Y:[[-5.]\n",
      " [ 5.]\n",
      " [-3.]\n",
      " [-8.]\n",
      " [-5.]]\n",
      "Z:-99.0\n"
     ]
    }
   ],
   "source": [
    "#inner product \n",
    "lim = 10\n",
    "M = 5\n",
    "\n",
    "X = tf.constant(np.random.randint(-lim,lim,[M,1]), tf.float32)\n",
    "Y = tf.constant(np.random.randint(-lim,lim,[M,1]), tf.float32)\n",
    "Z = tf.reduce_sum(X*Y)\n",
    "with tf.Session() as sess:\n",
    "    Z_o,Y_o,X_o = sess.run([Z,Y,X])\n",
    "    print('X:{}'.format(X_o))    \n",
    "    print('Y:{}'.format(Y_o))\n",
    "    print('Z:{}'.format(Z_o))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-vector product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: \n",
      "[[[ -7.  -4.  -6.  -5. -10.]]\n",
      "\n",
      " [[  0.   0.   0.  -7.   8.]]\n",
      "\n",
      " [[ -6.   2.   1.  -9.  -3.]]]\n",
      "Y: \n",
      "[[[ 7.]\n",
      "  [-4.]\n",
      "  [-2.]\n",
      "  [-7.]\n",
      "  [-6.]]]\n",
      "Tensor Z: \n",
      "[[-224.  128.   64.  224.  192.]\n",
      " [   7.   -4.   -2.   -7.   -6.]\n",
      " [-105.   60.   30.  105.   90.]]\n",
      "Tensor Zmatmul: \n",
      "[[74.]\n",
      " [ 1.]\n",
      " [29.]]\n"
     ]
    }
   ],
   "source": [
    "#Matrix-vector product \n",
    "lim = 10\n",
    "L = 3\n",
    "N = 5\n",
    "\n",
    "X = tf.constant(np.random.randint(-lim,lim,[L,1,N]), tf.float32)\n",
    "Y = tf.constant(np.random.randint(-lim,lim,[1,N,1]), tf.float32)\n",
    "\n",
    "Zmatmul = tf.matmul(X[:,0,:],Y[0,:,:]) #matmul to check validity    \n",
    "\n",
    "Z = tf.reduce_sum(Y*X,axis=2) #Y will now have shape [1,1,N]\n",
    "with tf.Session() as sess:\n",
    "    Z_o,Z_mmo,Xo,Yo = sess.run([Z,Zmatmul,X,Y])\n",
    "    print('X: \\n' + str(Xo))\n",
    "    print('Y: \\n' + str(Yo))\n",
    "    print('Tensor Z: \\n' + str(Z_o))\n",
    "    print('Tensor Zmatmul: \\n' + str(Z_mmo))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix-matrix product (not currently working)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 'Hello, World' with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reader at this point should understand the need to execute all operations within a session ```tf.Session()```.  There is a whole C++ backend to tensorflow that the python API is built around.  The python code specifies a \"Graph\" which is constructed, and this ultimately facilitates all model training and testing.  The session initiates the graph to manipulate the model and it's variables.  A basic (not being pretentious) neural network model known as a multi-layer perceptron (MLP) is demonstrated on the MNIST dataset.  After $20$ epochs, or $20$ times cycling through the data, our model should be performing at about $96\\%$ accuracy on the evaluation set (or holdout set, or test set).  It is common practice in ML to divide a dataset into sets like these as to prevent overfitting to the data and evaluate the model's ability to generalize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for epoch 1: 0.8813\n",
      "Accuracy for epoch 2: 0.9069\n",
      "Accuracy for epoch 3: 0.916\n",
      "Accuracy for epoch 4: 0.9246\n",
      "Accuracy for epoch 5: 0.9293\n",
      "Accuracy for epoch 6: 0.9346\n",
      "Accuracy for epoch 7: 0.9381\n",
      "Accuracy for epoch 8: 0.9396\n",
      "Accuracy for epoch 9: 0.9418\n",
      "Accuracy for epoch 10: 0.9448\n",
      "Accuracy for epoch 11: 0.9475\n",
      "Accuracy for epoch 12: 0.949\n",
      "Accuracy for epoch 13: 0.951\n",
      "Accuracy for epoch 14: 0.9522\n",
      "Accuracy for epoch 15: 0.9544\n",
      "Accuracy for epoch 16: 0.9554\n",
      "Accuracy for epoch 17: 0.9571\n",
      "Accuracy for epoch 18: 0.9585\n",
      "Accuracy for epoch 19: 0.9601\n",
      "Accuracy for epoch 20: 0.9609\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def np_one_hot(x,depth,axis=0):\n",
    "    '''helper function to convert np.array of ints or floats to a matrix of one-hot vectors'''\n",
    "    N = len(x)\n",
    "    x = x.astype('int')\n",
    "    xh = np.zeros((N,depth)).astype('float32')\n",
    "    xh[np.arange(N),x] = 1.\n",
    "    return xh\n",
    "\n",
    "def load_mnist():\n",
    "    '''load and preprocess MNIST dataset'''\n",
    "    (x_train,y_train), (x_eval,y_eval) = mnist.load_data() #load dataset\n",
    "    y_train = np_one_hot(y_train,NUM_CLASSES,axis = 1) #ie. 4 with 10 classes is [0,0,0,0,1,0,0,0,0]\n",
    "    y_eval = np_one_hot(y_eval,NUM_CLASSES,axis = 1)\n",
    "    x_train = np.reshape((x_train/255).astype('float32'),[60000,784]) #flatten to [60000,784] and normalize\n",
    "    x_eval = np.reshape((x_eval/255).astype('float32'),[10000,784]) #flatten to [10000,784] and normalize\n",
    "    return x_train,y_train,x_eval,y_eval\n",
    "\n",
    "def dense(x,input_dim,output_dim,name = 'dense'):\n",
    "    '''Basic dense layer without an activation  z = Wx + b'''\n",
    "    with tf.variable_scope(name):\n",
    "        kernel = tf.get_variable('kernel', shape = [input_dim, output_dim],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        bias = tf.get_variable('bias', shape = [output_dim],initializer=tf.contrib.layers.xavier_initializer())\n",
    "        x = tf.add(tf.matmul(x,kernel),bias)\n",
    "        return x\n",
    "    \n",
    "def mlp(x,reuse = False, name = 'mlp'):\n",
    "    '''model to be trained'''\n",
    "    with tf.variable_scope(name,reuse = reuse) as scope:\n",
    "        Bs,M = x.get_shape().as_list()\n",
    "        x = dense(x, input_dim = M, output_dim = NEURONS_PER_LAYER, name = 'dense0')\n",
    "        x = tf.nn.relu(x)\n",
    "        x = dense(x, input_dim = NEURONS_PER_LAYER, output_dim = NEURONS_PER_LAYER, name = 'dense1')\n",
    "        x = tf.nn.relu(x)\n",
    "        x = dense(x, input_dim = NEURONS_PER_LAYER, output_dim = NUM_CLASSES, name = 'out')\n",
    "        return x\n",
    "\n",
    "#Parameter Delcaration\n",
    "TRAINING_EPOCHS = 20\n",
    "TRAINING_SIZE = 60000\n",
    "EVAL_SIZE = 10000\n",
    "BATCH_SIZE = 100\n",
    "TRAIN_BATCHES_PER_EPOCH = int(TRAINING_SIZE/BATCH_SIZE)\n",
    "EVAL_BATCHES_PER_EPOCH = int(EVAL_SIZE/BATCH_SIZE)\n",
    "NEURONS_PER_LAYER = 512\n",
    "INPUT_DIM = 784 #(28 x 28)\n",
    "NUM_CLASSES = 10\n",
    "LEARNING_RATE = .01\n",
    "\n",
    "#load dataset\n",
    "x_train,y_train,x_eval,y_eval = load_mnist()\n",
    "\n",
    "# Placeholder tf Graph input\n",
    "X = tf.placeholder(\"float32\", [None,INPUT_DIM])\n",
    "Y = tf.placeholder(\"float32\", [None,NUM_CLASSES])\n",
    "\n",
    "#establish model, loss, and optimizer\n",
    "logits = mlp(X,name = 'mymlp')\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "#accuracy calculation functions\n",
    "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(logits), 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float32\"))\n",
    "\n",
    "#Initiate session and begin training/prediction\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    for epoch in range(TRAINING_EPOCHS):\n",
    "        train_cost = 0.\n",
    "        for batch in range(TRAIN_BATCHES_PER_EPOCH):\n",
    "            batch_x = x_train[batch*BATCH_SIZE : (batch + 1)*BATCH_SIZE]\n",
    "            batch_y = y_train[batch*BATCH_SIZE : (batch + 1)*BATCH_SIZE]\n",
    "            _,c = sess.run([train_op,loss_op], feed_dict = {X: batch_x, Y: batch_y})\n",
    "            train_cost += c/TRAIN_BATCHES_PER_EPOCH\n",
    "        print(\"Accuracy for epoch \" + str(epoch + 1) + \":\", accuracy.eval({X: x_eval, Y: y_eval}))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough of 'Hello, World' Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " That was a lot, with not a lot of comments, so let's take a step back and go through (almost) line-by-line and explain what just happened.  ```tf.reset_default_graph()``` is nice for clearing all graph variables.  Since a namespace is used, multiple runs of the script may confuse the RAM holding all the variable names, and will cause an error saying that those variables already exist.  This is particularly necessary when saving/loading weights from a previously trained execution.\n",
    " \n",
    " The first function ```np_one_hot``` is not core to anything, but a custom function to turn an array of integers or floats (integer values) into \"one-hot\" vectors.  Meaning that the $i^{th}$ index of a vector is $1$ and the rest are $0$.  The labels are interpreted this way in the cross entropy cost function (more later).\n",
    " \n",
    " In ```load_mnist``` the data is loaded from ```keras```' own library and preprocessed.  First the labels are converted to one-hot vectors and the data images themselves is flattend from a $28\\times 28$ matrix into a $784$ element vector.  All individual inputs to an MLP are vectors (the batch makes them a matrix), so it's traditional to flatten them like this. The data are also normalized (all pixel values $[0,1]$) from their 8-bit pixel values $[0,255]$.  Data normalization is a widely discussed topic in ML/DL, and publications have shown that it enables a model to converge faster CITATION.  It also tends to help your weights from blowing up.  There is also batch normalization which is a layer that can be added to tensorflow architectures.  \n",
    "\n",
    "The ```dense``` function describes the generic densely connected layer within a neural network. This document now takes a slight tangent to provide a formal overview of deriving the updates for the back-propagation algorithm \\cite{rumelhart1986learning} implemented for MLPs networks.  Throughout this each trainable layer will be referred to with the index $l$ denoting the $l^{th}$ layer of $L$ total layers and the $k^{th}$ neuron of $K_l$.  Each neuron is composed of a weight $w_l^k \\in \\mathbb{R}^{K_l-1 \\times 1}$ whose transpositions $(w_l^k)^T$ are the rows in matrix $W_l \\in \\mathbb{R}^{K_l\\times K_l-1}$ and bias term $b \\in \\mathbb{R}^{K_l\\times 1}$.   Eq. \\ref{eq:z} shows the pre-activation output $z_l \\in \\mathbb{R}^{K_l \\times 1}$ with input tensor $\\hat{y}_{l-1} \\in \\mathbb{R}^{K_l-1\\times 1}$ and output tensor $\\hat{y}_{l} \\in \\mathbb{R}^{K_l\\times 1}$.  Note that the input tensor at $l = 1$ is the data input $x\\in\\mathbb{R}^{N\\times 1}$, where $N$ denotes the original input dimension.  Eq. \\ref{eq:yzact} shows the activation function $f(z_l): \\mathbb{R}^{K_l\\times 1}\\to \\mathbb{R}^{K_l\\times 1}$ as an operation on $z_l$.\n",
    "\n",
    "\\begin{equation}\\label{eq:z}\n",
    "z_l = W_l\\hat{y}_{l-1} + b  = \\begin{bmatrix}W_l & b\\end{bmatrix} \\begin{bmatrix}\\hat{y}_{l-1} \\\\ 1\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\label{eq:yzact}\n",
    "\\hat{y}_l = f(z_l)\n",
    "\\end{equation}\n",
    "\n",
    "Each hidden layer utilizes a rectified linear unit (ReLU) Eq. \\ref{eq:relu}as it's activation function and an output layer softmax function \\ref{eq:softmax} for  $z_l$\\footnote{In a few cases it is necessary to refer to individual $k^{th}$ elements of a vector which are more often than not associated with the $k^{th}$ neuron, this will be denoted by $z_{l}[k]$ where $z_l \\in \\mathbb{R}^{K_l\\times 1}$.  The notation from \\cite{oppenheim2009dtsp} is used.}.  The reader may have observed that there is no softmax in the ```mlp``` function, this will be addressed when the cost function is discussed.  The $ReLU(\\cdot)$ presents an interesting case in that it cannot be directly differentiated.  However, piecewise it can be interpreted as Eq. \\ref{eq:drelu}, and the softmax derivative in \\ref{eq:dsoftmax}. These are commonly used activation functions \\cite{krizhevsky2012imagenet} \\cite{nair2010rectified}.\n",
    "\n",
    "\\begin{multicols}{2}\n",
    "\\begin{equation}\\label{eq:relu}\n",
    "ReLU(z_l) = max(0,z_l)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\label{eq:softmax}\n",
    "\\rho(z_l)= \\frac{e^{z_l}}{\\sum_{k = 1}^{K_l}e^{z_{l}[k]}}\n",
    "\\end{equation}\n",
    "\\end{multicols}\n",
    "\n",
    "\\begin{multicols}{2}\n",
    "\\begin{equation}\\label{eq:drelu}\n",
    "\\frac{\\partial ReLU(z_l)}{\\partial z_l} =  \\mathbbm{1}(z_l >0)\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\label{eq:dsoftmax}\n",
    "\\frac{\\partial\\rho(z_{l}[k])}{\\partial z_{l}[j]}= \\rho(z_{l}[k])(\\mathbbm{1}(j = k) - \\rho(z_{l}[j]))\n",
    "\\end{equation}\n",
    "\n",
    "\\end{multicols}\n",
    "\n",
    "These partial derivatives are necessary for deriving the update equations for each back-propagation iteration.  A cost/loss function of cross-entropy in Eq. \\ref{eq:crossentropy} will be used.  The apparent \"missing softmax\" is because both the cost function and output layer activation are all combined in the loss function that also facilitates the graidents: ```tf.nn.softmax_cross_entropy_with_logits_v2```.  $y\\in \\mathbb{R}^{K_L \\times 1}$ (and $\\therefore \\sum_{k=1}^{K_L}y_k = 1$) is the label for the one sample of data interpreted as a one-hot vector (all zeros except respective class index).  As a quick derivation denote relative entropy, or the Kullback-Leibler Divergence, and conclude that the given labels have no entropy $\\mathbbm{H}(y)=0$ based on both the facts that the labels are known and that numerically the metric is exactly $0$.  Next a formal derivation of the update equations is given, the reader can skip to the indicated section if they are primarily interested in the code itself.\n",
    "\n",
    "$\\textbf{SKIP IF NOT INTERESTED IN THE MATHEMATICAL DERIVATION}$\n",
    "\n",
    "\\begin{align*}\n",
    "\\mathbbm{D}(y|\\hat{y}) &= -\\sum_{k = 1}^{K_L}y[k]log\\left(\\frac{\\hat{y}_{L}[k]}{y[k]}\\right)\\\\ \\\n",
    "&= -\\sum_{k = 1}^{K_L}y[k]log(\\hat{y}_{L}[k]))+\\sum_{k = 1}^{K_L}y[k]log(y[k]) \\\\\n",
    "&=  -\\sum_{k = 1}^{K_L}y[k]log(\\hat{y}_{L}[k])) - \\mathbbm{H}(y) \\\\\n",
    "&= -\\sum_{k = 1}^{K_L}y[k]log(\\hat{y}_{L}[k]))\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\\label{eq:crossentropy}\n",
    "E = -\\sum_{k = 1}^{K_L}y[k]log(\\hat{y}_{L}[k])\n",
    "\\end{equation}\n",
    "\n",
    "This network has a final activation layer of softmax and finite hidden layers with $ReLU(\\cdot)$ activation.   In this case there are three steps in order to competely generalize all update equations;  this becomes an increasingly difficult problem as activations change layer-per-layer.  The first step is to derive the update equation for the output layer.  Rewrite $E$ as\n",
    "\n",
    "\\begin{equation}\\label{eq:costfuncz}\n",
    "E = -\\sum_{k = 1}^{K_L}y[k]log(\\hat{y}_{L}[k]) = -\\sum_{k=1}^{K_L} y[k](z_{L}[k]-log\\begin{pmatrix}\\sum_{i=1}^{K_L}e^{z_{L}[i]}\\end{pmatrix} \n",
    "\\end{equation}\n",
    "\n",
    "Then differentiate with respect to the weight $w_{k}^L$ using the chain rule.  The $j$ index represents the $j^{th}$ row of the update matrix.  Utilizing Eq. \\ref{eq:dsoftmax}:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_{k}^L} = \\frac{\\partial E}{\\partial \\hat{y}_{L}[k]}\\frac{\\partial \\hat{y}_{L}[k]}{\\partial z_{L}[k]}\\frac{\\partial z_{L}[k]}{\\partial w_{k}^L}$$\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial E}{\\partial \\hat{y}_{L}[k]}\\frac{\\partial \\hat{y}_{L}[k]}{\\partial z_{L}[k]}& = -\\sum_{k=1}^{K_L}y[k]\\begin{pmatrix}\\mathbbm{1}(j = k) - \\frac{exp(z_{L}[m])}{\\sum_{i=1}^{K_L}exp(z_{L}[i])}\\end{pmatrix}\\\\\n",
    "&=  -\\sum_{k=1}^{K_L}y[k]\\begin{pmatrix}\\mathbbm{1}(j = k) - \\hat{y}_{L}[k]\\end{pmatrix}\\\\\n",
    "&= -\\sum_{k=1}^{K_L}y[k]\\mathbbm{1}(j = k) + \\sum_{k=1}^{K_L}y[k]\\hat{y}_{L}[k]\\\\\n",
    "& = -y[k] + \\hat{y}_{L}[k]\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{equation}\\label{eq:gradEz}\n",
    "\\hat{y}_{L}[j] - y[j]\n",
    "\\end{equation}\n",
    "\n",
    "Trivially, using matrix differentiation rules\n",
    "\n",
    " $$\\frac{\\partial z_{L}[k]}{\\partial w_{k}^L} = \\hat{y}_{L-1}^T$$\n",
    "\n",
    "Hence, the $j^{th}$ row of the update matrix for the output layer $\\Delta W_L \\in \\mathbb{R}^{K_L \\times K_{L-1}+1}$ can be described by \n",
    "\n",
    "$$(\\hat{y}_{L}[j] - y[j])\\hat{y}_{L-1}^T$$\n",
    "\n",
    "A \"$1$\" is appended to the end of the input tensor to update the bias $b$ and by taking an outer product, in matrix notation form:\n",
    "\n",
    "\\begin{equation}\\label{eq:outputupdate}\n",
    "\\Delta W_L = (\\hat{y}_{L} - y)\\begin{bmatrix}\\hat{y}_{L-1}^T&1\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The attention now shifts to the $L-1$ layer, that is, the last hidden layer before the output layer with activation function $ReLU(z_{L-1})$.  The work to obtain Eq. \\ref{eq:gradEz} can be leveraged to start.  The required chain rule expression is as follows:\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial w_k^{L-1}} = \\frac{\\partial E}{\\partial \\hat{y}_{L}[k]}      \\frac{\\partial \\hat{y}_{L}[k]}{\\partial z_{L}[k]}    \\frac{\\partial z_{L}[k]}{\\partial \\hat{y}_{L-1}}    \\frac{\\partial \\hat{y}_{L-1}}{\\partial z_{L-1}[k]}  \\frac{\\partial z_{L-1}[k]}{\\partial w_k^{L-1}}   $$\n",
    "\n",
    "\n",
    "The first two partial derivatives were done for the first part.  From Eq. \\ref{eq:drelu}:\n",
    "\n",
    "$$\\frac{\\partial \\hat{y}_{L-1}}{\\partial z_{L-1}[k]} = \\mathbbm{1}(z_{L-1}[k] >0) $$\n",
    "\n",
    "and trivially,\n",
    "\n",
    "\\begin{align*}\n",
    "& \\frac{\\partial z_{L}[k]}{\\partial \\hat{y}_{L-1}}=(w_{L}^k)^T &\\frac{\\partial z_{L-1}[k]}{\\partial w_k^{L-1}} = \\hat{y}_{L-2}\n",
    "\\end{align*}\n",
    "\n",
    "The following expression is obtained in Eq. \\ref{eq:quasirow}, but cannot be directly inserted as a row in the update matrix $\\Delta W_{L-1} \\in \\mathbb{R}^{K_{L-1}+1 \\times K_{L-2}}$.    \n",
    "\n",
    "\\begin{equation}\\label{eq:quasirow}\n",
    "\\frac{\\partial E}{\\partial w_k^{L-1}} =\\hat{y}_{L-2}(\\hat{y}_{L}[k] - y[k])(w_{k}^L)^T \\mathbbm{1}(z_{L-1}[k] >0)\n",
    "\\end{equation}\n",
    "\n",
    "Define a diagonal matrix $P_{l-1} \\in \\mathbb{R}^{K_{l-1} \\times K_{l-1} }$ whose elements $P[k,k] =1$ if $z_{l-1}[k] >0$, and zeros otherwise. Then\n",
    "\n",
    "\\begin{equation}\\label{eq:updatehidden}\n",
    "\\Delta W^T_{L-1} = \\begin{bmatrix}\\hat{y}_{L-2}\\\\1\\end{bmatrix}(\\hat{y}_L^T - y^T)W_LP_{L-1}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\label{eq:generalupdate}\n",
    "\\Delta W^T_{l} = \\begin{bmatrix}\\hat{y}_{l-1}\\\\1\\end{bmatrix}(\\hat{y}_L^T - y^T)W_LP_{L-1}W_{L-1}P_{L-2} \\dots W_{l+1}P_{l}\n",
    "\\end{equation}\n",
    "\n",
    "The updates for stochastic gradient descent will look as follows in Eq. \\ref{eq:sgdu} for the $l^{th}$ layer.  $\\eta$ is the learning rate.  Batch updates are defined in Eq. \\ref{eq:batchu} where $B_s$ is the batch size.  Parallel computing would evaluate all gradients and weight updates simultaneously for $M$ forward passes.  Denote the tensor $\\Delta\\mathcal{W}_l \\in \\mathbb{R}^{B_s \\times K_{l+1} \\times K_{l + 1}}$ where the $m^{th}$ element is a 2D matrix of weights.\n",
    "\n",
    "\n",
    "\\begin{equation}\\label{eq:sgdu}\n",
    "\\begin{bmatrix}W_l & b\\end{bmatrix} = \\begin{bmatrix}W_l & b\\end{bmatrix} - \\eta \\Delta W_l\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\\label{eq:batchu}\n",
    "\\begin{bmatrix}W_l & b\\end{bmatrix} = \\begin{bmatrix}W_l & b\\end{bmatrix} - \\frac{\\eta}{B_s}\\sum_{m=1}^M \\Delta \\mathcal{W}_l[:,:,m]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{MATH DERIVATION ENDS HERE...}$\n",
    "\n",
    "All the derivations above fit nicely within the script as ```tf.train.GradientDescentOptimizer``` (aren't high-level APIs great?) since the output activation and cost function (softmax and cross-entropy) have already been designated.  Hence, when ```train_op``` is run in the session it calculates all these gradients and uses them to take step shown above.  These steps will take the \"steepest\" step towards what is hopefully a global minimum.  This minimization is what allows the model to fit to the data.\n",
    "\n",
    "The ```accuracy``` function is helper function that's used to calculate the evaluation accuracy to check in at each epoch of how well the model is doing.  Last but not least is the ```tf.Session()``` block which first initializes all variables ```tf.global_variables_initializer().run()```.  The outer loop goes through the entire training set (one ```epoch```) once each iteration and prints the accuracy on the evaluation set.  The inner loop is where the ```train_op``` is run in the session and updates the weights as described above.  Each inner loop iteration a batch is pulled from the full train dataset and set as the feed dictionary ```feed_dict```.  This is how the training routine knows what data to consume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lines 39 and 27 have use ```with``` to place the declared layers/kernels under a \"variable scope\".  This allows us to automatically declare new trainable variables when a new layer is added.  It also allows us to store these in the graph so that the trained weights can be reused (```reuse```) for evaluation purposes.  A lot more detail on variable scopes can be found in ```tensorflow``` documentation."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tensorflow_tutorial.ipynb",
   "provenance": [
    {
     "file_id": "1WWdRTBLzLue0UI0Gc7tn5vzrRoqmCv5l",
     "timestamp": 1562007574117
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
